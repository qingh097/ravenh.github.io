<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Huang(Raven) Huang</title>
  
  <meta name="author" content="Huang Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Huang Huang</name>
              </p>
              <p>I am a third year PhD student in EECS at UC Berkeley, advised by Prof. Ken Goldberg. My research focuses on robot learning for manipulation including lateral access mechanical search, deformable object manipulation and grasping. Recently I have also worked on the representation learning between tactile and vision. 
                I got my M.S. degree in ME at UT Austin advised by Prof. Luis Sentis, working on control and modelling of human in exoskeleton.
              </p>
              <p style="text-align:center">
                <a href="mailto:huangr@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=N_KDBGcAAAAJ">Google Scholar</a> 
                <!-- &nbsp/&nbsp -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/raven_img.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/raven_img.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

           <!-- TACVIS -->
           <tr onmouseout="tacvis_stop()" onmouseover="tacvis_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='tacvis_under'>
                  <img src="images/tacvis_under.png" width='180'/>
                </div>
                <img src="images/tacvis_splash.png" width='180' id='tacvis_splash'/>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function tacvis_start() {
                  document.getElementById('tacvis_splash').style.opacity = "0";
                  document.getElementById('tacvis_under').style.opacity = "1";
                }

                function tacvis_stop() {
                  document.getElementById('tacvis_splash').style.opacity = "1";
                  document.getElementById('tacvis_under').style.opacity = "0";
                }
                tacvis_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <papertitle>Learning Self-Supervised Representations from Vision and Touch
                  for Active Sliding Perception of Deformable Surfaces</papertitle>
              <br>
              <!-- AUTHORS -->
              Justin Kerr*, <b>Huang Huang</b>*, Albert Wilcox, Ryan Hoque, Jeffrey Ichnowski, Roberto Calandra, and Ken Goldberg,
              <small>*Equal contribution</small>
              <br>
              <!-- CONFERENCE -->
              <em>ICRA</em> 2023 (Submitted)
              <p></p>
              <!-- SUMMARY -->
              We learn a self-supervised representation cross tactile and vistion using contrastive loss. We collect vision-tacitile pairs in a self-supervised way in real. The learned representation is utilized in the downstream active perception tasks without fine-tuning.
            </td>
          </tr>

          <!-- staxray -->
          <tr onmouseout="staxray_stop()" onmouseover="staxray_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='staxray_destackr'>
                  <img src="images/staxray_destackr.png" width='180'/>
                </div>
                <img src="images/staxray_restack.png" width='180' id='staxray_restack'/>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function staxray_start() {
                  document.getElementById('staxray_restack').style.opacity = "0";
                  document.getElementById('staxray_destack').style.opacity = "1";
                }

                function staxray_stop() {
                  document.getElementById('staxray_restack').style.opacity = "1";
                  document.getElementById('staxray_destack').style.opacity = "0";
                }
                staxray_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->  
                <a href="https://sites.google.com/berkeley.edu/stax-ray">
                <papertitle>Mechanical Search on Shelves with Efficient
                  Stacking and Destacking of Objects</papertitle>
                </a>
              <br>
              <!-- AUTHORS -->
                <b>Huang Huang</b>*, Letian Fu*, Michael Danielczuk, Chung Min Kim,
                Zachary Tam, Jeffrey Ichnowski, Anelia Angelova, Brian Ichter, and
                Ken Goldberg, <small>*Equal contribution</small>
              <br>
              <!-- CONFERENCE -->
              <em>ISRR</em> 2022, <a href="https://arxiv.org/pdf/2207.02347.pdf">arXiv</a>
              <p></p>
              <!-- SUMMARY -->
              We develop two policies for lateral access mechanical search with stacked objects. Both policies utilize stacking and destacking actions and can reveal the
              target object with 82–100% success in simulation outperforming the baseline by up to 66%, and can achieve 67–100% success in physical experiments.
            </td>
          </tr>

          <!-- EVO-NERF -->
          <tr onmouseout="evonerf_stop()" onmouseover="evonerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='evonerf_under'>
                  <img src="images/evonerf_early_stop.png" width='180'/>
                </div>
                <video  width=115% height=100% muted autoplay loop id="evonerf_splash">
                  <source src="images/evonerf_splash.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function evonerf_start() {
                  document.getElementById('evonerf_splash').style.opacity = "0";
                  document.getElementById('evonerf_under').style.opacity = "1";
                }

                function evonerf_stop() {
                  document.getElementById('evonerf_splash').style.opacity = "1";
                  document.getElementById('evonerf_under').style.opacity = "0";
                }
                evonerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <a href="https://sites.google.com/view/evo-nerf">
                <papertitle>Evo-NeRF: Evolving NeRF for Sequential Robot Grasping</papertitle>
              </a>
              <br>
              <!-- AUTHORS -->
              Justin Kerr, Letian Fu, <b>Huang Huang</b>, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, Ken Goldberg
              <br>
              <!-- CONFERENCE -->
              <em>CoRL</em> 2022, <b>Oral Presentation</b>, <a href="https://openreview.net/forum?id=Bxr45keYrf">OpenReview</a>
              <p></p>
              <!-- SUMMARY -->
              NeRF functions as a real-time, updateable scene reconstruction for rapidly grasping table-top transparent objects.
              Geometry regularization speeds and improves scene geometry, and a NeRF-adapted grasping network learns to ignore floaters.
            </td>
          </tr>

          <!-- LUV -->
          <tr onmouseout="luv_stop()" onmouseover="luv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <!-- IMAGES -->
              <div class="one">
                <div class="two" id='luv_splash'>
                  <video  width=115% height=100% muted autoplay loop>
                    <source src="images/random_start.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video>
                </div>
                <!-- <img src='images/luv_splash.png' width="180" id="luv_under"> -->
                <video  width=115% height=100% muted autoplay loop id="luv_under">
                  <source src="images/luv_smoothing.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <!-- SCRIPT FOR HOVERING -->
              <script type="text/javascript">
                function luv_start() {
                  document.getElementById('luv_splash').style.opacity = "0";
                  document.getElementById('luv_under').style.opacity = "1";
                }

                function luv_stop() {
                  document.getElementById('luv_splash').style.opacity = "1";
                  document.getElementById('luv_under').style.opacity = "0";
                }
                luv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- TITLE -->
              <a href="https://sites.google.com/berkeley.edu/luv">
                <papertitle>All You Need is LUV: Unsupervised Collection of Labeled Images using Invisible UV Fluorescent Indicators</papertitle>
              </a>
              <br>
              <!-- AUTHORS -->
              Brijen Thananjeyan*, Justin Kerr*, <b>Huang Huang</b>, Joseph E. Gonzalez, Ken Goldberg
              <br>
              * Equal contribution
              <br>
              <!-- CONFERENCE -->
              <em>IROS</em> 2022, <a href="https://arxiv.org/abs/2203.04566">arXiv</a>
              <p></p>
              <!-- SUMMARY -->
              <p>Fluorescent paint enables inexpensive (<$200) and self-supervised data collection of dense image annotations without altering objects' appearance. We demonstrate its broad applicability to several manipulation domains.</p>
            </td>
          </tr>
          
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
